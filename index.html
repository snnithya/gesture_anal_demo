<!DOCTYPE html>
<html lang="en">

<head>
	<meta name="generator" content="Hugo 0.84.0" />
  <meta charset="utf-8">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <title>Hugo Whisper Theme</title>
  <meta name="viewport" content="width=device-width, initial-scale=1">
  
<meta name="description" content="Hugo Whisper is a documentation theme built with Hugo." />
<meta property="og:title" content="Hugo Whisper Theme" />
<meta property="og:type" content="website" />
<meta property="og:url" content="https://hugo-whisper.netlify.app" />
<meta property="og:image" content="https://raw.githubusercontent.com/JugglerX/hugo-whisper-theme/master/images/tn.png" />
<meta property="og:description" content="Hugo Whisper is a documentation theme built with Hugo." />
<meta name="twitter:card" content="summary" />
<meta name="twitter:site" content="@zerostaticio" />
<meta name="twitter:creator" content="@zerostaticio" /> 
  <link rel="icon" href="https://snnithya.github.io/gesture_analysis_demo/favicon.png">

  
  
  <link rel="stylesheet" href="/gesture_analysis_demo/css/style.min.ba7c70780b880261622082b3bf81b5168bc8411e7467705ff3ab192994cd2227.css">
  

  

</head>

<body class='page page-home'>
  <div id="main-menu-mobile" class="main-menu-mobile">
  <ul>
    
    
    <li class="menu-item-home active">
      <a href="/gesture_analysis_demo/">
        <span>Home</span>
      </a>
    </li>
    
    <li class="menu-item-supplementary material">
      <a href="/gesture_analysis_demo/suppl/">
        <span>Supplementary Material</span>
      </a>
    </li>
    
    <li class="menu-item-about">
      <a href="/gesture_analysis_demo/about/">
        <span>About</span>
      </a>
    </li>
    
  </ul>
</div>
  <div class="wrapper">
    <div class='header'>
    <div class="container">
        <div class="logo">
            <a href="https://snnithya.github.io/gesture_analysis_demo"><img alt="Logo" src="/gesture_analysis_demo/images/logo.svg" style="width:250px; padding-bottom: 10px;" /></a>
        </div>
        <div class="logo-mobile">
            <a href="https://snnithya.github.io/gesture_analysis_demo"><img alt="Logo" src="/gesture_analysis_demo/images/logo-mobile.svg" /></a>
        </div>
        <div id="main-menu" class="main-menu">
  <ul>
    
    
    <li class="menu-item-home active">
      <a href="/gesture_analysis_demo/">
        <span>Home</span>
      </a>
    </li>
    
    <li class="menu-item-supplementary material">
      <a href="/gesture_analysis_demo/suppl/">
        <span>Supplementary Material</span>
      </a>
    </li>
    
    <li class="menu-item-about">
      <a href="/gesture_analysis_demo/about/">
        <span>About</span>
      </a>
    </li>
    
  </ul>
</div> <button id="toggle-main-menu-mobile" class="hamburger hamburger--slider" type="button">
  <span class="hamburger-box">
    <span class="hamburger-inner"></span>
  </span>
</button>
    </div>
</div>

    
    
    
    

    
     

<div class="strip">
    <div class="container pt-4">
        <div class="row">
            <div class="col-12">
                <h1 class="title">Raga classification from vocal performance using multimodal analysis</h1>
            </div>
        </div>
    </div>
</div>

<div class="strip">
    <div class="container pt-4 pb-16">
        <div class="row">
            <div class="col-12">
                <h2 class="subtitle">Abstract</h2>
                <div class="content" style="text-align: justify;">
                     <p>Work on musical gesture and embodied cognition suggests a rich complementarity between audio and movement information in musical performance. Pose estimation algorithms now make it possible (in contrast to traditional Motion Capture) to collect rich movement information from unconstrained performances of indefinite length. Vocal performances of Indian art music  offer the opportunity to carry out multimodal analysis using this information, combining musicianâ€™s body movements (i.e. pose and gesture data) with audio features. In this work we investigate raga identification from 12 s excerpts from a dataset of 3 singers and 9 ragas using the combination of audio and visual representations that are each semantically salient on their own. While gesture based classification is relatively weak by itself, we show that combining latent representations from the pre-trained unimodal networks can surpass the already high performance obtained by audio features.</p>
 
                </div>
                <a class="button button-primary mb-2" href="/gesture_analysis_demo/paper">
          Read The Paper
        </a>
            </div>
        </div>
    </div>
</div>

<div class="strip">
    <div class="container pt-4 pb-4">
        <div class="row justify-content-center">
            <div class="col-8 col-md-6">
                <div class="home-image">
                    <img src="/gesture_analysis_demo/images/3-singers-cj.png" />
                </div>
            </div>
        </div>
    </div>
</div>
 
    
  </div>

  <div class="sub-footer">
    <div class="container">
        <div class="row">
            <div class="col-12">
                <div class="sub-footer-inner">
                    <ul>
                        <li class="zerostatic"><a href="https://www.ee.iitb.ac.in/course/~daplab/">Digital Audio Processing Lab, IIT Bombay</a></li>
                    </ul>
                </div>
            </div>
        </div>
    </div>
</div>

  

  
  

  
  <script type="text/javascript" src="/gesture_analysis_demo/js/scripts.min.eaf147370baecdd07c022597db631f99cab1c9cd6479de586f30327a568d6a0f.js"></script>
  

  
  
  
    
  


</body>

</html>
